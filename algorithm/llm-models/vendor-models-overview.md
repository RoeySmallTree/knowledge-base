# LLM Vendor Models Overview
*Structured output compatible models only - organized by vendor for selection*

---

## Alibaba
**1 model**

- **Tongyi DeepResearch 30B A3B** - Agentic model optimized for long-horizon deep research tasks with 30B total/3B active parameters

---

## AllenAI
**4 models**

- **Olmo 3.1 32B Think** - Deep reasoning model, 32B params, Apache 2.0, fully open-source
- **Olmo 3 32B Think** - Earlier version of 32B reasoning model
- **Olmo 3 7B Instruct** - Instruction-tuned 7B model for everyday tasks
- **Olmo 3 7B Think** - Reasoning-focused 7B variant

---

## Anthropic
**3 models**

- **Claude Opus 4.5** - Frontier reasoning model for complex software engineering and agentic workflows
- **Claude Sonnet 4.5** - Advanced Sonnet for real-world agents and coding, 1M context
- **Claude Opus 4.1** - Updated flagship with 74.5% on SWE-bench, extended thinking up to 64K tokens

---

## Arcee AI
**2 models**

- **Trinity Mini** - 26B total/3B active MoE, efficient reasoning over 131K context
- **Trinity Mini (free)** - Free tier of Trinity Mini

---

## Baidu
**1 model**

- **ERNIE 4.5 21B A3B** - Lightweight MoE (21B total/3B active) for reasoning, math, coding

---

## ByteDance Seed
**2 models**

- **Seed 1.6 Flash** - Ultra-fast multimodal deep thinking, 256K context
- **Seed 1.6** - General-purpose multimodal with adaptive deep thinking, 256K context

---

## Cohere
**4 models**

- **Command R7B 12-2024** - Latest 7B model with strong RAG capabilities
- **Command R 08-2024** - Enterprise-focused model for retrieval-augmented generation
- **Command R+ 08-2024** - Enhanced version with better reasoning
- **Command R 03-2024** - Earlier stable release

---

## DeepCogito
**3 models**

- **Cogito R1 Preview** - Reasoning model with transparent thinking process
- **Cogito R1 Preview (free)** - Free tier
- **Cogito R1 Preview 32B** - Larger 32B variant

---

## DeepSeek
**12 models**

- **DeepSeek R1** - Flagship reasoning model, exceptional value
- **DeepSeek R1 Distill Llama 70B** - Distilled version on Llama 70B base
- **DeepSeek R1 Distill Qwen 32B** - Distilled on Qwen 32B, efficient reasoning
- **DeepSeek R1 Distill Qwen 14B** - 14B distilled variant
- **DeepSeek R1 Distill Qwen 7B** - Smallest distilled version
- **DeepSeek R1 Distill Llama 8B** - 8B Llama-based distill
- **DeepSeek R1 (free)** - Free tier of flagship
- **DeepSeek V3** - Latest general-purpose model
- **DeepSeek Chat** - Conversational variant
- **DeepSeek Coder** - Specialized for coding tasks
- **DeepSeek V2.5** - Previous generation flagship
- **DeepSeek V2.5 (free)** - Free tier

---

## EssentialAI
**1 model**

- **Rnj-1 8B** - 8B model focused on programming, math, and scientific reasoning

---

## Google
**19 models**

- **Gemini 3 Flash Preview** - High-speed thinking model for agentic workflows, 1M context
- **Gemini 3 Pro Image Preview** - Advanced image generation/editing (Nano Banana Pro)
- **Gemini 3 Pro Preview** - Flagship frontier model, 1M context, multimodal
- **Gemini 2.5 Flash Image** - Image-focused Flash variant (Nano Banana)
- **Gemini 2.5 Flash Preview 09-2025** - State-of-the-art for reasoning, coding, math
- **Gemini 2.5 Flash Lite Preview 09-2025** - Ultra-low latency lightweight reasoning
- **Gemini 2.5 Flash Image Preview** - Image preview variant
- **Gemini 2.5 Flash Lite** - Production lightweight reasoning model
- **Gemini 2.5 Flash** - Production workhorse for advanced reasoning
- **Gemini 2.0 Flash Thinking Exp 1219** - Experimental thinking variant
- **Gemini 2.0 Flash Exp** - Experimental Flash release
- **Gemini 2.0 Flash** - Latest production Flash model
- **Gemini Exp 1206** - Experimental release from Dec 2024
- **Gemini 1.5 Flash 8B** - Efficient 8B variant
- **Gemini 1.5 Flash** - Previous generation Flash
- **Gemini 1.5 Pro** - Previous generation Pro
- **Gemini Pro 1.5** - Alternative naming for 1.5 Pro
- **Gemini Flash 1.5** - Alternative naming for 1.5 Flash
- **Gemini Flash 1.5 8B** - Alternative naming for 8B variant

---

## Gryphe
**1 model**

- **MythoMax 13B** - Creative writing and fantasy/RP-focused model

---

## Inception
**2 models**

- **Mercury 3 Thinking** - Reasoning-optimized Mercury variant
- **Mercury 3** - General-purpose efficient model

---

## KwaiPilot
**2 models**

- **KwaiPilot 3 32B** - Coding-optimized 32B model
- **KwaiPilot 3 7B** - Smaller 7B coding variant

---

## Meta Llama
**6 models**

- **Llama 3.3 70B Instruct** - Latest 70B instruction-tuned model
- **Llama 3.1 405B Instruct** - Massive 405B flagship model
- **Llama 3.1 70B Instruct** - Popular 70B instruction model
- **Llama 3.1 8B Instruct** - Efficient 8B variant
- **Llama 3.2 3B Instruct** - Tiny 3B instruction model
- **Llama 3.2 1B Instruct** - Ultra-small 1B model

---

## Microsoft
**1 model**

- **Phi 4** - Latest efficient small model from Microsoft

---

## MiniMax
**2 models**

- **MiniMax M2.1** - Lightweight 10B model for coding and agents
- **MiniMax M2** - Previous generation compact model

---

## MistralAI
**27 models**

- **Mistral Large 3 2512** - Flagship MoE (41B active/675B total), Apache 2.0
- **Mistral Small 3.2 2512** - Balanced performance/efficiency
- **Ministral 3 14B 2512** - Largest Ministral, vision-capable
- **Ministral 3 8B 2512** - Balanced tiny model with vision
- **Ministral 3 3B 2512** - Smallest Ministral with vision
- **Devstral 2 2512** - 123B coding specialist
- **Devstral 2 2512 (free)** - Free tier
- **Mistral Small Creative** - Experimental creative writing model
- **Mistral Large 2** - Previous flagship
- **Mistral Small 2** - Previous small variant
- **Pixtral Large 2** - Multimodal large model
- **Pixtral 12B 2409** - 12B vision-language model
- **Mistral Nemo** - Efficient 12B model
- **Codestral Mamba** - Mamba-based coding model
- **Mistral 7B Instruct** - Classic 7B instruction model
- **Mixtral 8x7B Instruct** - MoE with 8 experts
- **Mixtral 8x22B Instruct** - Larger MoE variant
- **Mixtral 8x7B** - Base MoE model
- **Mixtral 8x22B** - Base larger MoE
- Plus 8 more variants (free tiers, dated versions)

---

## MoonshotAI
**5 models**

- **Kimi 3** - Latest with massive context windows
- **Moonshot V1 128K** - 128K context variant
- **Moonshot V1 32K** - 32K context variant
- **Moonshot V1 8K** - 8K context variant
- **Kimi 2.5** - Previous generation

---

## Neversleep
**2 models**

- **Llama 3.1 Lumimaid 70B** - Creative writing Llama fine-tune
- **Llama 3.1 Lumimaid 8B** - Smaller creative variant

---

## Nex AGI
**1 model**

- **Nex AGI 1.0** - AGI-aligned reasoning model

---

## NousResearch
**4 models**

- **Hermes 3 Llama 3.1 405B** - Massive 405B Hermes variant
- **Hermes 3 Llama 3.1 70B** - Popular 70B Hermes
- **Hermes 3 Llama 3.2 3B** - Tiny 3B Hermes
- **Hermes 2 Theta Llama 3 70B** - Alternative 70B variant

---

## NVIDIA
**3 models**

- **Nemotron 3 Nano 30B A3B** - 30B MoE optimized for NVIDIA hardware
- **Nemotron 3 Nano 30B A3B (free)** - Free tier
- **Llama 3.1 Nemotron 70B Instruct** - NVIDIA-tuned Llama 70B

---

## OpenAI
**53 models**

- **GPT-5.2 Chat** - Fast lightweight GPT-5 for chat (Instant)
- **GPT-5.2 Pro** - Most advanced with agentic coding, 400K context
- **GPT-5.2** - Latest frontier model with adaptive reasoning
- **GPT-4.5 Turbo** - Enhanced GPT-4 with better reasoning
- **GPT-4 Turbo** - Previous generation flagship
- **GPT-4o** - Omni model with multimodal capabilities
- **GPT-4o Mini** - Efficient small variant
- **o1** - Reasoning-focused model
- **o1 Mini** - Smaller reasoning variant
- **o1 Preview** - Preview of o1 series
- **o3 Mini** - Latest mini reasoning model
- Plus 43 more variants (dated versions, fine-tuned, extended context)

---

## OpenGVLab
**1 model**

- **InternVL 2.5 78B** - Vision-language model, 78B params

---

## Prime Intellect
**1 model**

- **INTELLECT-3** - 106B MoE (12B active) trained via decentralized compute

---

## Qwen
**31 models**

- **QwQ 32B Preview** - Reasoning-focused preview
- **Qwen 2.5 Coder 32B Instruct** - Coding specialist 32B
- **Qwen 2.5 72B Instruct** - Flagship 72B instruction model
- **Qwen 2.5 32B Instruct** - Balanced 32B variant
- **Qwen 2.5 14B Instruct** - Efficient 14B model
- **Qwen 2.5 7B Instruct** - Popular 7B variant
- **Qwen 2.5 3B Instruct** - Tiny 3B model
- **Qwen 2.5 1.5B Instruct** - Ultra-small 1.5B
- **Qwen 2.5 0.5B Instruct** - Smallest 0.5B variant
- **QVQ 72B Preview** - Vision-question-answering 72B
- **Qwen 2 VL 72B Instruct** - Vision-language 72B
- **Qwen 2 VL 7B Instruct** - Vision-language 7B
- Plus 19 more variants (free tiers, dated versions, specialized)

---

## Sao10k
**3 models**

- **Llama 3.1 Euryale 70B** - Creative/RP Llama fine-tune
- **Llama 3 Stheno 8B** - Smaller creative variant
- **Fimbulvetr 11B v2** - Specialized creative model

---

## StepFun AI
**1 model**

- **Step 2 16K** - General-purpose model with 16K context

---

## Tencent
**1 model**

- **Hunyuan Turbo** - Enterprise-focused Chinese model

---

## TheDrummer
**3 models**

- **Llama 3.1 Sthenorm 70B** - Uncensored creative Llama fine-tune
- **Llama 3.1 Sthenorm 8B** - Smaller uncensored variant
- **Rocinante 12B** - Specialized creative model

---

## TngTech
**2 models**

- **Llama 3.1 TNG 70B** - EU-based DeepSeek fine-tune
- **Llama 3.1 TNG 8B** - Smaller EU variant

---

## Undi95
**1 model**

- **Llama 3.1 Toppy 8B** - RP-specialized Llama fine-tune

---

## Venice
**1 model**

- **Venice Llama 3.3 70B** - Privacy-focused Llama variant

---

## X.AI
**6 models**

- **Grok 2 1212** - Latest Grok with real-time X/Twitter integration
- **Grok 2 Vision 1212** - Vision-capable Grok
- **Grok 2** - Previous generation
- **Grok 2 Vision** - Previous vision variant
- **Grok Beta** - Beta release
- **Grok Vision Beta** - Beta vision release

---

## Z.AI
**7 models**

- **GLM 4.7** - Latest flagship with enhanced programming
- **GLM 4.6V** - Vision-language variant
- **GLM 4.6** - Previous generation with 200K context
- **GLM 4.6 (exacto)** - Exacto variant
- **GLM 4.5V** - Vision-language foundation model
- **GLM 4.5** - Agent-focused foundation model
- **GLM 4.5 Air** - Lightweight agent-centric variant

---

**Total: 219 models across 36 vendors**
