[
  {"id": "#1", "fallback": "#19", "reason": "Both ultra-fast multimodal Flash-tier models"},
  {"id": "#2", "fallback": "#3", "reason": "Both Chinese enterprise MoE with tool calling"},
  {"id": "#3", "fallback": "#196", "reason": "Both Chinese enterprise with function calling"},
  {"id": "#5", "fallback": "#170", "reason": "Both top-tier agentic coding MoE architectures"},
  {"id": "#6", "fallback": "#9", "reason": "Same GLM vision family, similar capabilities"},
  {"id": "#7", "fallback": "#10", "reason": "Same GLM family, similar MoE architecture"},
  {"id": "#8", "fallback": "#7", "reason": "Variant fallback to base GLM-4.6"},
  {"id": "#9", "fallback": "#184", "reason": "Both MoE vision models with GUI agent capabilities"},
  {"id": "#10", "fallback": "#172", "reason": "Both large MoE general-purpose instruction models"},
  {"id": "#11", "fallback": "#120", "reason": "Both efficient MoE for cost-sensitive deployments"},
  {"id": "#12", "fallback": "#138", "reason": "Both fast flagship models with agentic search"},
  {"id": "#14", "fallback": "#141", "reason": "Both #1-tier flagship reasoning models globally"},
  {"id": "#19", "fallback": "#67", "reason": "Both ultra-efficient high-throughput nano-tier"},
  {"id": "#20", "fallback": "#138", "reason": "Both balanced fast models with tool calling"},
  {"id": "#21", "fallback": "#54", "reason": "Both stable flagship models for production"},
  {"id": "#26", "fallback": "#35", "reason": "Both open ~27-30B models for local deployment"},
  {"id": "#33", "fallback": "#205", "reason": "Both 7-8B open models with tool use"},
  {"id": "#35", "fallback": "#188", "reason": "Both ~30B efficient agentic MoE models"},
  {"id": "#36", "fallback": "#33", "reason": "Both sub-10B efficient edge models"},
  {"id": "#40", "fallback": "#135", "reason": "Both top-tier enterprise flagships (#1-2 globally)"},
  {"id": "#41", "fallback": "#91", "reason": "Both frontier agentic coding models"},
  {"id": "#42", "fallback": "#136", "reason": "Both conversational flagship models"},
  {"id": "#48", "fallback": "#49", "reason": "Same Deep Research family"},
  {"id": "#49", "fallback": "#188", "reason": "Both lightweight deep research models"},
  {"id": "#54", "fallback": "#42", "reason": "Both unified flagship models with adaptive reasoning"},
  {"id": "#55", "fallback": "#66", "reason": "Both cost-efficient production mini-tier"},
  {"id": "#56", "fallback": "#67", "reason": "Both ultra-fast nano-tier models"},
  {"id": "#57", "fallback": "#7", "reason": "Both open-weight ~100B+ MoE with tool calling"},
  {"id": "#59", "fallback": "#11", "reason": "Both lightweight open MoE for edge/local"},
  {"id": "#60", "fallback": "#63", "reason": "Same o3 family, standard compute tier"},
  {"id": "#62", "fallback": "#64", "reason": "Same o4-mini family, standard effort"},
  {"id": "#63", "fallback": "#143", "reason": "Both frontier reasoning models with tool access"},
  {"id": "#64", "fallback": "#142", "reason": "Both efficient reasoning models"},
  {"id": "#65", "fallback": "#42", "reason": "Both long-context instruction-following flagships"},
  {"id": "#66", "fallback": "#20", "reason": "Both balanced efficient production models"},
  {"id": "#67", "fallback": "#95", "reason": "Both ultra-efficient nano-tier for classification"},
  {"id": "#91", "fallback": "#96", "reason": "Both Mistral ecosystem enterprise agentic coding"},
  {"id": "#94", "fallback": "#36", "reason": "Both 8-9B efficient edge reasoning models"},
  {"id": "#95", "fallback": "#205", "reason": "Both tiny efficient models for edge deployment"},
  {"id": "#96", "fallback": "#123", "reason": "Both large MoE enterprise flagships"},
  {"id": "#98", "fallback": "#132", "reason": "Both ~70B enterprise workhorses with reasoning"},
  {"id": "#99", "fallback": "#183", "reason": "Both code completion/FIM specialists"},
  {"id": "#102", "fallback": "#120", "reason": "Both 24-26B enterprise agentic with tool calling"},
  {"id": "#110", "fallback": "#189", "reason": "Both large multimodal for document analysis"},
  {"id": "#118", "fallback": "#5", "reason": "Both 600B+ MoE agentic coding models"},
  {"id": "#119", "fallback": "#194", "reason": "Both 8B agentic coding with function calling"},
  {"id": "#120", "fallback": "#26", "reason": "Both ~26-27B open models for enterprise"},
  {"id": "#122", "fallback": "#169", "reason": "Both ultra-deep competition-grade reasoning"},
  {"id": "#123", "fallback": "#126", "reason": "Same DeepSeek V3 family, stable fallback"},
  {"id": "#126", "fallback": "#202", "reason": "Both large open MoE agentic models"},
  {"id": "#128", "fallback": "#132", "reason": "Same R1 reasoning family"},
  {"id": "#130", "fallback": "#181", "reason": "Both 32B reasoning specialists"},
  {"id": "#131", "fallback": "#212", "reason": "Both 14B math/reasoning specialists"},
  {"id": "#132", "fallback": "#134", "reason": "Both 70-106B dense reasoning models"},
  {"id": "#134", "fallback": "#191", "reason": "Both ~70-106B models with tool calling and reasoning"},
  {"id": "#135", "fallback": "#14", "reason": "Both #1-2 globally ranked flagships"},
  {"id": "#136", "fallback": "#12", "reason": "Both top-tier efficient coding flagships"},
  {"id": "#137", "fallback": "#136", "reason": "Same Claude family, newer capable model"},
  {"id": "#138", "fallback": "#139", "reason": "Same Grok Fast family"},
  {"id": "#139", "fallback": "#55", "reason": "Both cost-efficient fast production models"},
  {"id": "#140", "fallback": "#91", "reason": "Both fast agentic coding models"},
  {"id": "#141", "fallback": "#63", "reason": "Both flagship deep reasoning models"},
  {"id": "#142", "fallback": "#166", "reason": "Both efficient reasoning models with transparent thinking"},
  {"id": "#143", "fallback": "#128", "reason": "Both chain-of-thought reasoning with self-correction"},
  {"id": "#147", "fallback": "#170", "reason": "Both top agentic coding models with tool orchestration"},
  {"id": "#159", "fallback": "#6", "reason": "Both vision MoE with deep reasoning capabilities"},
  {"id": "#160", "fallback": "#189", "reason": "Both large vision-language instruction models"},
  {"id": "#166", "fallback": "#192", "reason": "Both ~24-30B MoE with dual-mode thinking"},
  {"id": "#168", "fallback": "#166", "reason": "Same Qwen3 30B model variant"},
  {"id": "#169", "fallback": "#60", "reason": "Both ultra-deep maximum-compute reasoning models"},
  {"id": "#170", "fallback": "#91", "reason": "Both frontier agentic coding specialists"},
  {"id": "#172", "fallback": "#96", "reason": "Both large MoE instruction-following flagships"},
  {"id": "#177", "fallback": "#166", "reason": "Both 30-32B Qwen models with dual-mode reasoning"},
  {"id": "#181", "fallback": "#177", "reason": "Both 32B Qwen-based reasoning specialists"},
  {"id": "#183", "fallback": "#201", "reason": "Both code completion/generation specialists"},
  {"id": "#184", "fallback": "#159", "reason": "Both large vision-language models with reasoning"},
  {"id": "#188", "fallback": "#120", "reason": "Both efficient agentic models with tool use"},
  {"id": "#189", "fallback": "#184", "reason": "Both large open multimodal models"},
  {"id": "#191", "fallback": "#193", "reason": "Same Nous 70B family, user-aligned"},
  {"id": "#192", "fallback": "#102", "reason": "Both 24B with function calling and reasoning"},
  {"id": "#193", "fallback": "#204", "reason": "Both 70B generalist instruction models"},
  {"id": "#194", "fallback": "#205", "reason": "Both 8B Llama-based with tool use"},
  {"id": "#195", "fallback": "#191", "reason": "Both uncensored/user-aligned permissive models"},
  {"id": "#196", "fallback": "#199", "reason": "Both Chinese enterprise tool-calling flagships"},
  {"id": "#199", "fallback": "#2", "reason": "Both Chinese flagship MoE with search integration"},
  {"id": "#201", "fallback": "#140", "reason": "Both fast code generation/completion models"},
  {"id": "#202", "fallback": "#206", "reason": "Both 400B-scale open multimodal flagships"},
  {"id": "#203", "fallback": "#202", "reason": "Same Llama 4 family, larger model"},
  {"id": "#204", "fallback": "#98", "reason": "Both ~70B production workhorse instruction models"},
  {"id": "#205", "fallback": "#94", "reason": "Both 8B efficient edge models with multilingual"},
  {"id": "#206", "fallback": "#96", "reason": "Both 400-700B enterprise flagship MoE"},
  {"id": "#212", "fallback": "#36", "reason": "Both efficient 9-14B STEM/reasoning models"},
  {"id": "#213", "fallback": "#214", "reason": "Same Euryale family, spatial-aware roleplay"},
  {"id": "#214", "fallback": "#191", "reason": "Both 70B creative/roleplay with user alignment"},
  {"id": "#215", "fallback": "#218", "reason": "Both 8-13B creative roleplay balanced models"},
  {"id": "#216", "fallback": "#215", "reason": "Both 8B roleplay specialists, slop-free"},
  {"id": "#217", "fallback": "#216", "reason": "Both NeverSleep roleplay, similar training"},
  {"id": "#218", "fallback": "#219", "reason": "Both 13B Llama 2 roleplay merges"},
  {"id": "#219", "fallback": "#213", "reason": "Both top-tier creative storytelling/roleplay"}
]